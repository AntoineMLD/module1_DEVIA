{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper-paramètres Clés de XGBoost<br><br>\n",
    "\n",
    "**Learning Rate (taux d'apprentissage)** : Affecte la contribution de chaque arbre à la prédiction finale. Un taux plus faible et plus d'arbres peuvent améliorer la généralisation.<br>\n",
    "**N_estimators** : Nombre d'arbres séquentiels à construire. Plus il y a d'arbres, plus le modèle peut être précis, mais attention au surajustement.<br>\n",
    "**Max_depth** : Profondeur maximale de chaque arbre. Contrôle la complexité de chaque arbre, avec un risque plus élevé de surajustement pour des valeurs plus grandes.<br>\n",
    "**Min_child_weight** : Somme minimale des poids d'instance nécessaire dans un enfant. Utilisé pour contrôler le surajustement.<br>\n",
    "**Subsample** : Fraction des échantillons à utiliser pour entraîner chaque arbre. Un faible subsample peut conduire à un sous-ajustement, tandis qu'un subsample élevé à un surajustement.<br>\n",
    "**Colsample_bytree** : Fraction des caractéristiques à utiliser pour chaque arbre. Une valeur plus faible peut améliorer la performance en prévenant le surajustement.<br><br>\n",
    "\n",
    "**XGBoost** :<br>\n",
    "Extrêmement efficace, flexible et portable. Idéal pour les grands ensembles de données.<br>\n",
    "Supporte à la fois le boosting séquentiel et le parallélisme pour une formation rapide.<br>\n",
    "Offre un contrôle avancé sur l'ajustement du modèle et l'ingénierie des fonctionnalités.<br>\n",
    "Bien adapté pour les compétitions de data science et les problèmes complexes.<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meilleurs paramètres:  {'learning_rate': 0.01, 'max_depth': 6, 'n_estimators': 300}\n",
      "Meilleur score:  0.82630667578036\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Charger les données\n",
    "data = pd.read_csv('datasets/titanic.csv')\n",
    "\n",
    "# Prétraitement des données\n",
    "label_encoder = LabelEncoder()\n",
    "data['sex'] = label_encoder.fit_transform(data['sex'].fillna('NA'))\n",
    "data['embarked'] = label_encoder.fit_transform(data['embarked'].fillna('NA'))\n",
    "data['age'] = data['age'].fillna(data['age'].median())\n",
    "data['fare'] = data['fare'].fillna(data['fare'].median())\n",
    "data['pclass'] = data['pclass'].fillna(data['pclass'].median())\n",
    "data['sibsp'] = data['sibsp'].fillna(data['sibsp'].median())\n",
    "data['parch'] = data['parch'].fillna(data['parch'].median())\n",
    "\n",
    "# Sélectionner les caractéristiques et la cible\n",
    "features = ['pclass', 'sex', 'age', 'sibsp', 'parch', 'fare', 'embarked']\n",
    "target = 'survived'\n",
    "\n",
    "X = data[features]\n",
    "y = data[target].ffill()  # Remplacer les NaN dans la cible par la valeur précédente\n",
    "\n",
    "# Diviser les données\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Définir le modèle XGBoost et le GridSearch\n",
    "xgb_model = XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'max_depth': [3, 4, 5, 6],\n",
    "    \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(xgb_model, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Afficher les meilleurs paramètres et le meilleur score\n",
    "print(\"Meilleurs paramètres: \", grid_search.best_params_)\n",
    "print(\"Meilleur score: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez allez voir la documentation du XGBoost Classifier et jouer avec les hyperparamètres , vous n'avez pour le moment que les 3 principaux."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
