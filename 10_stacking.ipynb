{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "___\n",
    "___\n",
    "# *STACKING*\n",
    "___\n",
    "___\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le stacking, ou empilement en français, est une technique d'ensemble (ensemble learning) utilisée dans l'apprentissage automatique. L'idée principale du stacking est de combiner les prédictions de plusieurs modèles de base pour améliorer la performance globale du modèle.\n",
    "\n",
    "Voici une explication simple du processus de stacking :\n",
    "\n",
    "1. **Entraînement des Modèles de Base :** Vous commencez par entraîner plusieurs modèles de base sur votre ensemble de données.\n",
    "\n",
    "2. **Prédictions des Modèles de Base :** Une fois les modèles de base entraînés, vous les utilisez pour faire des prédictions sur les données de test (ou sur une partie de vos données d'entraînement que vous avez réservée à cet effet).\n",
    "\n",
    "3. **Création d'un Modèle Méta :** Ensuite, vous utilisez ces prédictions en tant que caractéristiques d'entrée pour un modèle méta (également appelé méta-modèle ou modèle de niveau supérieur). Ce modèle méta est généralement un modèle plus simple, comme une régression logistique pour les problèmes de classification ou une régression linéaire pour les problèmes de régression.\n",
    "\n",
    "4. **Entraînement du Modèle Méta :** Vous entraînez le modèle méta sur les prédictions des modèles de base.\n",
    "\n",
    "5. **Prédictions Finales :** Une fois le modèle méta entraîné, vous l'utilisez pour faire des prédictions finales sur de nouvelles données.\n",
    "\n",
    "L'idée sous-jacente est que le modèle méta peut apprendre à combiner les forces des modèles de base, compensant ainsi leurs faiblesses individuelles. Le stacking est particulièrement utile lorsque les modèles de base ont des performances complémentaires sur différentes parties de l'ensemble de données ou dans différentes régions de l'espace des caractéristiques.\n",
    "\n",
    "**/!\\ attention /!\\ :** Il est important de noter que le stacking nécessite une attention particulière pour éviter le surajustement, et le choix judicieux des modèles de base est crucial pour son succès. En pratique, le stacking peut considérablement améliorer la performance prédictive par rapport à l'utilisation d'un seul modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Base Models:\n",
      "Random Forest: 0.9000\n",
      "Gradient Boosting: 0.9150\n",
      "k-Nearest Neighbors: 0.8100\n",
      "\n",
      "Accuracy of Stacked Model: 0.9150\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Génération de données fictives pour l'exemple\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=20, random_state=42)\n",
    "\n",
    "# Division des données en ensembles d'entraînement et de test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Entraînement des modèles de base\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "gb_model = GradientBoostingClassifier(random_state=42)\n",
    "knn_model = KNeighborsClassifier()\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "gb_model.fit(X_train, y_train)\n",
    "knn_model.fit(X_train, y_train)\n",
    "\n",
    "# Prédictions des modèles de base sur l'ensemble de test\n",
    "rf_predictions = rf_model.predict(X_test)\n",
    "gb_predictions = gb_model.predict(X_test)\n",
    "knn_predictions = knn_model.predict(X_test)\n",
    "\n",
    "# Création du nouvel ensemble de données avec les prédictions des modèles de base\n",
    "stacked_X = [rf_predictions, gb_predictions, knn_predictions]\n",
    "stacked_X = np.array(stacked_X).T\n",
    "\n",
    "# Entraînement du modèle méta (régression logistique dans cet exemple)\n",
    "meta_model = LogisticRegression(random_state=42)\n",
    "meta_model.fit(stacked_X, y_test)\n",
    "\n",
    "# Prédictions finales avec le modèle méta\n",
    "stacked_predictions = meta_model.predict(stacked_X)\n",
    "\n",
    "# Évaluation de la performance\n",
    "accuracy_base_models = {\n",
    "    'Random Forest': accuracy_score(y_test, rf_predictions),\n",
    "    'Gradient Boosting': accuracy_score(y_test, gb_predictions),\n",
    "    'k-Nearest Neighbors': accuracy_score(y_test, knn_predictions)\n",
    "}\n",
    "\n",
    "accuracy_stacked_model = accuracy_score(y_test, stacked_predictions)\n",
    "\n",
    "print(\"Accuracy of Base Models:\")\n",
    "for model, acc in accuracy_base_models.items():\n",
    "    print(f\"{model}: {acc:.4f}\")\n",
    "\n",
    "print(f\"\\nAccuracy of Stacked Model: {accuracy_stacked_model:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
